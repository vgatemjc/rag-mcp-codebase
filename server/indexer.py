import os, glob, json, uuid
from typing import List, Dict, Tuple
import httpx
import numpy as np
# ê°€ì •: vecdb, chunking, graph, settingsëŠ” ì •ìƒì ìœ¼ë¡œ importë¨
from vecdb import VecDB 
from chunking import chunk_code
from graph import list_functions
from settings import settings
import logging

logger = logging.getLogger(__name__)

# íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì–¸ì–´ ë§¤í•‘
EXT_LANG = {
    ".py": "python", ".ts": "typescript", ".tsx": "typescript", ".js": "javascript",
    ".java": "java", ".go": "go", ".cpp": "cpp", ".cc": "cpp", ".hpp": "cpp", ".c": "c",
    ".rs": "rs", ".kt": "kt", ".swift": "swift"
}

# TEI ì„œë²„ì˜ ìµœëŒ€ ë°°ì¹˜ í¬ê¸° (ë¡œê·¸ì—ì„œ 64ë¡œ í™•ì¸ë¨)
MAX_EMBEDDING_BATCH_SIZE = 64 

class Indexer:
    def __init__(self):
        self.db = VecDB(settings.qdrant_url)
        # _embedëŠ” ë™ê¸° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
        self.dim = self._embed(["dimension check"]).shape[1] 
        self.db.ensure_collection(settings.collection_code, self.dim)
        self.db.ensure_collection(settings.collection_funcs, self.dim)
        logger.info(f"Indexer ì´ˆê¸°í™” ì™„ë£Œ. ë²¡í„° ì°¨ì›: {self.dim}")

    async def _embed_async(self, texts: List[str]) -> np.ndarray:
        """ë¹„ë™ê¸° ì„ë² ë”© í˜¸ì¶œ (FastAPI ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)"""
        async with httpx.AsyncClient(timeout=60) as cli:
            r = await cli.post(f"{settings.tei_url}/embed", json={"inputs": texts})
            r.raise_for_status()
            # TEIê°€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
            return np.array(r.json(), dtype=np.float32)

    def _embed(self, texts: List[str]) -> np.ndarray:
        """ë™ê¸° ì„ë² ë”© í˜¸ì¶œ (ì´ˆê¸°í™” ë° ë™ê¸° í•¨ìˆ˜ì—ì„œ ì‚¬ìš©)"""
        r = httpx.post(f"{settings.tei_url}/embed", json={"inputs": texts}, timeout=60)
        r.raise_for_status()
        # TEIê°€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
        return np.array(r.json(), dtype=np.float32)

    def _embed_in_batches(self, texts: List[str]) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ë¥¼ MAX_EMBEDDING_BATCH_SIZE ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.
        """
        all_embeddings = []
        n = len(texts)
        
        for i in range(0, n, MAX_EMBEDDING_BATCH_SIZE):
            batch = texts[i:i + MAX_EMBEDDING_BATCH_SIZE]
            logger.info(f"Embedding batch {i//MAX_EMBEDDING_BATCH_SIZE + 1} of {len(batch)} items (Total: {n})")
            
            # _embed í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë°°ì¹˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì„ë² ë”©
            try:
                batch_emb = self._embed(batch)
                all_embeddings.append(batch_emb)
            except httpx.HTTPStatusError as e:
                logger.error(f"Embedding failed for batch starting at index {i}: {e}")
                # ì„ë² ë”©ì— ì‹¤íŒ¨í•˜ë©´ í•´ë‹¹ ë°°ì¹˜ëŠ” ê±´ë„ˆë›¸ ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬ (ì—¬ê¸°ì„œëŠ” ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì•¼ í•¨)
                raise # 413 ì˜¤ë¥˜ì˜ ê²½ìš° ì´ ì˜ˆì™¸ê°€ ë°œìƒí•  ê²ƒì…ë‹ˆë‹¤.
        # ğŸŒŸ FIX: ëª¨ë“  ì„ë² ë”© ë°°ì—´ì„ ë‹¨ì¼ NumPy ë°°ì—´ë¡œ ì—°ê²°í•˜ì—¬ ë°˜í™˜ ğŸŒŸ
        
        if not all_embeddings:
            # ì„ë² ë”© ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
            # ì´ˆê¸°í™” ì‹œì ì— `self.dim`ì„ ì´ë¯¸ í™•ì¸í–ˆìœ¼ë¯€ë¡œ, ì°¨ì› 0ì¸ ë°°ì—´ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            return np.zeros((0, self.dim), dtype=np.float32) 
            
        return np.concatenate(all_embeddings, axis=0)

    def _detect_lang(self, path: str) -> str:
        return EXT_LANG.get(os.path.splitext(path)[1].lower(), "")

    def _iter_files(self, root: str) -> List[str]:
        ignore = {".git", "node_modules", "build", "dist", ".venv", "venv", "__pycache__"}
        for dirpath, dirnames, filenames in os.walk(root):
            # íƒìƒ‰ì—ì„œ ì œì™¸í•  ë””ë ‰í† ë¦¬ ì„¤ì •
            dirnames[:] = [d for d in dirnames if d not in ignore]
            for f in filenames:
                p = os.path.join(dirpath, f)
                # ì–¸ì–´ë¥¼ ê°ì§€í•  ìˆ˜ ìˆëŠ” íŒŒì¼ë§Œ í¬í•¨
                if self._detect_lang(p):
                    yield p

    def index_repo(self, repo_path: str, repo_name: str):
        # ì „ì²´ repoì— ëŒ€í•´ ë‹¨ì¼ ë°°ì¹˜ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ë¦¬ìŠ¤íŠ¸
        code_ids, code_vecs, code_payloads = [], [], []
        func_ids, func_vecs, func_payloads = [], [], []

        for path in self._iter_files(repo_path):
            lang = self._detect_lang(path)
            # pathê°€ repo_pathë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ relpath ê³„ì‚°
            relative_path = os.path.relpath(path, repo_path)
            
            logger.info(f"--> [INDEX] Processing file: {relative_path} (Lang: {lang})")
            
            try:
                # 'r' ëª¨ë“œë¡œ ì—´ê³  ì¸ì½”ë”© ì˜¤ë¥˜ ë¬´ì‹œ
                with open(path, 'r', encoding='utf-8', errors='ignore') as fp:
                    text = fp.read()
            except IOError as e:
                logger.error(f"Error reading file {path}: {e}")
                continue # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™

            # 1. ì²­í¬ ì²˜ë¦¬ (Chunks)
            for (start, end, chunk) in chunk_code(text, max_lines=200, overlap=20):
                eid = str(uuid.uuid4())
                code_ids.append(eid)
                code_payloads.append({
                    "repo": repo_name, "file": relative_path,
                    "start": start, "end": end, "lang": lang, "type": "chunk"
                })
                code_vecs.append(chunk)

            # 2. í•¨ìˆ˜ ì²˜ë¦¬ (Functions)
            # list_functionsëŠ” bytesë¥¼ ë°›ìœ¼ë¯€ë¡œ ì¸ì½”ë”©
            funcs = list_functions(lang, text.encode('utf-8'))
            lines = text.splitlines()
            
            for name, fstart, fend in funcs:
                eid = str(uuid.uuid4())
                func_ids.append(eid)
                
                # --- ë²„ê·¸ 2 ìˆ˜ì •: í•¨ìˆ˜ ë¼ì¸ì„ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ë¬¸ìì—´ë¡œ ê²°í•© ---
                # ë¼ì¸ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ ì¸ë±ìŠ¤ëŠ” fstart-1ë¶€í„° fend-1ê¹Œì§€ì…ë‹ˆë‹¤.
                func_body = "\n".join(lines[fstart-1:fend]) 
                
                # í•¨ìˆ˜ ì´ë¦„ê³¼ ë³¸ë¬¸ì„ ê²°í•©í•˜ì—¬ ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„±
                func_text_for_embed = f"{name}\n{func_body}"
                
                func_payloads.append({
                    "repo": repo_name, "file": relative_path,
                    "start": fstart, "end": fend, "lang": lang, 
                    "name": name, "type": "function"
                })
                func_vecs.append(func_text_for_embed)

        # ----------------------------------------------------
        # --- ìˆ˜ì •ëœ ë¶€ë¶„: _embed_in_batches ì‚¬ìš© ---
        # ----------------------------------------------------
        
        indexed_chunks = 0
        indexed_functions = 0
        
        # ì²­í¬ ì„ë² ë”© ë° ì—…ì„œíŠ¸
        if code_vecs:
            logger.info(f"Starting batch embedding for {len(code_vecs)} code chunks...")
            code_emb = self._embed_in_batches(code_vecs) # ë°°ì¹˜ ì„ë² ë”© ì‚¬ìš©
            self.db.upsert(settings.collection_code, code_ids, code_emb, code_payloads)
            indexed_chunks = len(code_vecs)
        else:
            logger.warning("No code chunks found for indexing.")
        
        # í•¨ìˆ˜ ì„ë² ë”© ë° ì—…ì„œíŠ¸
        if func_vecs:
            logger.info(f"Starting batch embedding for {len(func_vecs)} functions...")
            func_emb = self._embed_in_batches(func_vecs) # ë°°ì¹˜ ì„ë² ë”© ì‚¬ìš©
            self.db.upsert(settings.collection_funcs, func_ids, func_emb, func_payloads)
            indexed_functions = len(func_vecs)
        else:
            logger.warning("No functions found for indexing.")
            
        return {"chunks": indexed_chunks, "functions": indexed_functions}