
services:
  # --------------------------
  # Qdrant - Vector DB
  # --------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    network_mode: "host"
    volumes:
      - ./rag-db:/qdrant/storage

#  embedding: # vllm
#    image: vllm/vllm-openai:latest
#    container_name: vllm
#    restart: unless-stopped
#    network_mode: "host"
#    ipc: host
#    deploy:  # <-- Uncomment this whole block
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all  # Or 'all'
#              capabilities: [gpu]
#    environment:
#      - HUGGING_FACE_HUB_ENABLE_HF_TRANSFER=1
#    command: >-
#      --model nomic-ai/nomic-embed-code
#      --dtype float16
#      --quantization bitsandbytes
#      --max-model-len 8192
#      --gpu-memory-utilization 0.3
#      --max-num-seqs 64
#      --port 8003
#    volumes:
#      - ./model_cache:/root/.cache/huggingface

  # --------------------------
  # TEI - Embedding 서버
  embedding:
     image: ghcr.io/huggingface/text-embeddings-inference:86-1.8
     container_name: tei
     restart: unless-stopped
     network_mode: "host"
     deploy:
       resources:
         reservations:
           devices:
             - capabilities: [gpu]
     environment:
 #      - MODEL_ID=BAAI/bge-m3
       - MODEL_ID=nomic-ai/CodeRankEmbed
#       - MODEL_ID=nomic-ai/nomic-embed-code
       - DTYPE=float16
       - HUGGING_FACE_HUB_ENABLE_HF_TRANSFER=1
       - MAX_CLIENT_BATCH_SIZE=64
       - MAX_CONCURRENT_REQUESTS=8
       - PAYLOAD-LIMIT=100000000
       - MAX_BATCH_TOKENS=32768       # 모델 max_input_length
       - AUTO_TRUNCATE=true              # 초과되면 자동 truncate
     volumes:
       - ./model_cache:/data

  # --------------------------
  # MCP Server - Custom RAG 전처리
  # --------------------------
  mcp-server:
    build:
      context: ./server
      dockerfile: Dockerfile.mcp
    container_name: mcp-server
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - embedding
      - rag-server
    environment:
      QDRANT_URL: "${QDRANT_ENDPOINT}"
      EMB_URL: "${EMB_ENDPOINT}"
      COLLECTION_CODE: "code_chunks"
      COLLECTION_FUNCS: "functions"
      REPO_ROOT: "/workspace/myrepo"
      MCP_PORT: "8083"
      FASTMCP_SERVER_PORT: 8083
      OLLAMA_URL: "${OLLAMA_ENDPOINT}"
    command: >
      sh -c "
      echo '⏳ Waiting for EMEDDING SERVER to be ready...' &&
      timeout 60s sh -c 'until curl -sf ${EMB_ENDPOINT}/health; do sleep 2; done' &&
      echo '✅ TEI is ready, starting mcp-server...' &&
      python git_rag_mcp.py
      "
    volumes:
      - ./server:/app
      - ${HOST_REPO_PATH}:/workspace/myrepo:rw

  # --------------------------
  # RAG Server - API 제공
  # --------------------------
  rag-server:
    build:
      context: ./server
      dockerfile: Dockerfile.rag
    container_name: rag-server
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - embedding
    environment:
      QDRANT_URL: "${QDRANT_ENDPOINT}"
      EMB_BASE_URL: "${EMB_ENDPOINT}"
      EMB_MODEL: "nomic-ai/CodeRankEmbed"
      OLLAMA_URL: "${OLLAMA_ENDPOINT}"
      DIM: "768"
    volumes:
      - ./server:/app
      - ${HOST_REPO_PATH}:/workspace/myrepo:rw
    command: >
      sh -c "
      echo '⏳ Waiting for EMBEDDING SERVER READY to be ready...' &&
      timeout 60s sh -c 'until curl -sf ${EMB_ENDPOINT}/health; do sleep 2; done' &&
      echo '✅ TEI is ready, starting rag-server...' &&
      git config --global --add safe.directory /workspace/myrepo/finding_food/ &&
      uvicorn git_rag_api:app --host 0.0.0.0 --port 8000
      "

  # --------------------------
  # Open WebUI v0.6.32
  # --------------------------
  webui:
    image: ghcr.io/open-webui/open-webui:0.6.36
    container_name: open-webui
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - embedding
      - rag-server
    environment:
      OLLAMA_BASE_URL: "${OLLAMA_ENDPOINT}"  # Ollama URL (TEI 대체 가능)
      VECTOR_DB: qdrant
      QDRANT_URI: "${QDRANT_ENDPOINT}"
      ENABLE_MCP: true
      ENABLE_RAG: true
      EMBEDDING_ENGINE: vllm
      RAG_EMBEDDING_ENGINE: vllm
      RAG_EMBEDDING_MODEL: nomic-ai/nomic-embed-code
      RAG_EMBEDDING_API_BASE: "${RAG_SERVER_ENDPOINT}"
#      RAG_EMBEDDING_DIM: 1024
      WEBUI_AUTH: "false"
      ENABLE_MODEL_SELECTOR: "true"
      RAG_ALLOWED_FILE_EXTENSIONS: pdf,docx,md,txt,json,yaml,yml,toml,csv,html,css,js,ts,jsx,tsx,py,java,c,cpp,h,go,sh,dockerfile,rs,log
    volumes:
      - ./webui_data:/app/backend/data

# --------------------------
# Docker Volumes
# --------------------------
volumes:
  rag-db:
  model_cache:
  webui_data:
