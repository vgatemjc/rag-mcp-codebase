
services:
  # --------------------------
  # Qdrant - Vector DB
  # --------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    network_mode: "host"
    volumes:
      - ./rag-db:/qdrant/storage

  # --------------------------
  # TEI - Embedding 서버
  # --------------------------
  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:86-1.8
    container_name: tei
    restart: unless-stopped
    network_mode: "host"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
#      - MODEL_ID=BAAI/bge-m3
      - MODEL_ID=nomic-ai/CodeRankEmbed
#      - MODEL_ID=nomic-ai/nomic-embed-code
      - DTYPE=float16
      - HUGGING_FACE_HUB_ENABLE_HF_TRANSFER=1
      - MAX_CLIENT_BATCH_SIZE=64
      - MAX_CONCURRENT_REQUESTS=8
    volumes:
      - ./model_cache:/data

  # --------------------------
  # MCP Server - Custom RAG 전처리
  # --------------------------
  mcp-server:
    build:
      context: ./server
      dockerfile: Dockerfile.mcp
    container_name: mcp-server
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - tei
    environment:
      QDRANT_URL: "http://127.0.0.1:6333"
      TEI_URL: "http://127.0.0.1:80"
      COLLECTION_CODE: "code_chunks"
      COLLECTION_FUNCS: "functions"
      REPO_ROOT: "/workspace"
      MCP_PORT: "8083"
      FASTMCP_SERVER_PORT: 8083
      OLLAMA_URL: "http://127.0.0.1:11434"
    command: >
      sh -c "
      echo '⏳ Waiting for TEI to be ready...' &&
      timeout 60s sh -c 'until curl -sf http://127.0.0.1:80/health; do sleep 2; done' &&
      echo '✅ TEI is ready, starting mcp-server...' &&
      python mcp_server.py
      "
    volumes:
      - ./server:/app
      - /home/vgate/workspace/open_ai_gym/finding_food:/workspace/myrepo:ro

  # --------------------------
  # RAG Server - API 제공
  # --------------------------
  rag-server:
    build:
      context: ./server
      dockerfile: Dockerfile.rag
    container_name: rag-server
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - tei
      - mcp-server
    environment:
      QDRANT_URL: "http://127.0.0.1:6333"
      TEI_BASE_URL: "http://127.0.0.1:80"
      TEI_MODEL: "nomic-ai/CodeRankEmbed"
      OLLAMA_URL: "http://127.0.0.1:11434"
      DIM: "768"
    volumes:
      - ./server:/app
      - /home/vgate/workspace/git_test:/workspace/myrepo:rw
#      - /home/vgate/workspace/open_ai_gym/finding_food:/workspace/myrepo:rw
    command: >
      sh -c "
      echo '⏳ Waiting for TEI to be ready...' &&
      timeout 60s sh -c 'until curl -sf http://127.0.0.1:80/health; do sleep 2; done' &&
      echo '✅ TEI is ready, starting rag-server...' &&
      uvicorn git_rag_api:app --host 0.0.0.0 --port 8000
      "

  # --------------------------
  # Open WebUI v0.6.32
  # --------------------------
  webui:
    image: ghcr.io/open-webui/open-webui:0.6.34
    container_name: open-webui
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - tei
      - rag-server
    environment:
      OLLAMA_BASE_URL: http://127.0.0.1:11434  # Ollama URL (TEI 대체 가능)
      VECTOR_DB: qdrant
      QDRANT_URI: http://127.0.0.1:6333
      ENABLE_MCP: true
      ENABLE_RAG: true
      EMBEDDING_ENGINE: tei
      RAG_EMBEDDING_ENGINE: tei
      TEI_URL: http://127.0.0.1:80  # 호스트에서 TEI 접근 시
      RAG_EMBEDDING_MODEL: BAAI/bge-m3
      RAG_EMBEDDING_API_BASE: http://127.0.0.1:80
#      RAG_EMBEDDING_DIM: 1024
      WEBUI_AUTH: "false"
      ENABLE_MODEL_SELECTOR: "true"
      RAG_ALLOWED_FILE_EXTENSIONS: pdf,docx,md,txt,json,yaml,yml,toml,csv,html,css,js,ts,jsx,tsx,py,java,c,cpp,h,go,sh,dockerfile,rs,log
    volumes:
      - ./webui_data:/app/backend/data

# --------------------------
# Docker Volumes
# --------------------------
volumes:
  rag-db:
  model_cache:
  webui_data:
