
services:
  # --------------------------
  # Qdrant - Vector DB
  # --------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    network_mode: "host"
    volumes:
      - ./rag-db:/qdrant/storage

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    restart: unless-stopped
    network_mode: "host"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_ENABLE_HF_TRANSFER=1
    command: >-
      --model nomic-ai/nomic-embed-code
      --dtype float16
      --quantization bitsandbytes
      --task embedding
      --max-model-len 8192
      --gpu-memory-utilization 0.9
      --max-num-seqs 64
    volumes:
      - ./model_cache:/root/.cache/huggingface

  # --------------------------
  # TEI - Embedding 서버
  # --------------------------
#   tei:
#     image: ghcr.io/huggingface/text-embeddings-inference:86-1.8
#     container_name: tei
#     restart: unless-stopped
#     network_mode: "host"
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - capabilities: [gpu]
#     environment:
# #      - MODEL_ID=BAAI/bge-m3
#       - MODEL_ID=nomic-ai/CodeRankEmbed
# #      - MODEL_ID=nomic-ai/nomic-embed-code
#       - DTYPE=float16
#       - HUGGING_FACE_HUB_ENABLE_HF_TRANSFER=1
#       - MAX_CLIENT_BATCH_SIZE=64
#       - MAX_CONCURRENT_REQUESTS=8
#       - PAYLOAD-LIMIT=100000000
#     volumes:
#       - ./model_cache:/data

  # --------------------------
  # MCP Server - Custom RAG 전처리
  # --------------------------
  mcp-server:
    build:
      context: ./server
      dockerfile: Dockerfile.mcp
    container_name: mcp-server
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - vllm
      - rag-server
    environment:
      QDRANT_URL: "http://127.0.0.1:6333"
      TEI_URL: "http://127.0.0.1:80"
      COLLECTION_CODE: "code_chunks"
      COLLECTION_FUNCS: "functions"
      REPO_ROOT: "/workspace"
      MCP_PORT: "8083"
      FASTMCP_SERVER_PORT: 8083
      OLLAMA_URL: "http://127.0.0.1:11434"
    command: >
      sh -c "
      echo '⏳ Waiting for TEI to be ready...' &&
      timeout 60s sh -c 'until curl -sf http://127.0.0.1:80/health; do sleep 2; done' &&
      echo '✅ TEI is ready, starting mcp-server...' &&
      python git_rag_mcp.py
      "
    volumes:
      - ./server:/app
#      - /home/vgate/workspace/open_ai_gym/finding_food:/workspace/myrepo:ro
      - /home/vgate/workspace/open_ai_gym:/workspace:rw

  # --------------------------
  # RAG Server - API 제공
  # --------------------------
  rag-server:
    build:
      context: ./server
      dockerfile: Dockerfile.rag
    container_name: rag-server
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - vllm
    environment:
      QDRANT_URL: "http://127.0.0.1:6333"
      TEI_BASE_URL: "http://127.0.0.1:80"
      TEI_MODEL: "nomic-ai/CodeRankEmbed"
      OLLAMA_URL: "http://127.0.0.1:11434"
      DIM: "768"
    volumes:
      - ./server:/app
#      - /home/vgate/workspace/git_test:/workspace/myrepo:rw
      - /home/vgate/workspace/open_ai_gym:/workspace/myrepo:rw
    command: >
      sh -c "
      echo '⏳ Waiting for TEI to be ready...' &&
      timeout 60s sh -c 'until curl -sf http://127.0.0.1:80/health; do sleep 2; done' &&
      echo '✅ TEI is ready, starting rag-server...' &&
      git config --global --add safe.directory /workspace/myrepo/finding_food/ &&
      uvicorn git_rag_api:app --host 0.0.0.0 --port 8000
      "

  # --------------------------
  # Open WebUI v0.6.32
  # --------------------------
  webui:
    image: ghcr.io/open-webui/open-webui:0.6.34
    container_name: open-webui
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      - qdrant
      - vllm
      - rag-server
    environment:
      OLLAMA_BASE_URL: http://127.0.0.1:11434  # Ollama URL (TEI 대체 가능)
      VECTOR_DB: qdrant
      QDRANT_URI: http://127.0.0.1:6333
      ENABLE_MCP: true
      ENABLE_RAG: true
      EMBEDDING_ENGINE: vllm
      RAG_EMBEDDING_ENGINE: vllm
      TEI_URL: http://127.0.0.1:80  # 호스트에서 TEI 접근 시
      RAG_EMBEDDING_MODEL: nomic-ai/nomic-embed-code
      RAG_EMBEDDING_API_BASE: http://127.0.0.1:80
#      RAG_EMBEDDING_DIM: 1024
      WEBUI_AUTH: "false"
      ENABLE_MODEL_SELECTOR: "true"
      RAG_ALLOWED_FILE_EXTENSIONS: pdf,docx,md,txt,json,yaml,yml,toml,csv,html,css,js,ts,jsx,tsx,py,java,c,cpp,h,go,sh,dockerfile,rs,log
    volumes:
      - ./webui_data:/app/backend/data

# --------------------------
# Docker Volumes
# --------------------------
volumes:
  rag-db:
  model_cache:
  webui_data:
